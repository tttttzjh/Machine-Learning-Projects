# -*- coding: utf-8 -*-
"""California Housing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BeCm2KeEYXcEvs1_Zk7Q0dHZE1iFA6ap
"""

from sklearn.datasets import fetch_california_housing
import pandas as pd

# as_frame=True, the data is returned as a Pandas DataFrame (with column names)
housing = fetch_california_housing(as_frame=True)
df = housing.frame
df.head()

from sklearn.model_selection import train_test_split

# axis=0 is by rows, axis=1 is by columns
X = df.drop("MedHouseVal", axis=1) # all features except MedHouseVal
y = df["MedHouseVal"] # target column

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

lr = LinearRegression()
lr.fit(X_train, y_train)

y_pred = lr.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.4f}")
print(f"R²:   {r2:.4f}")

# Polynomial Regression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline


# degree=2 means adding all squares and pairwise interactions
# Example: if features = [x1, x2], it will generate [x1, x2, x1², x1*x2, x2²]
# include_bias=False means don’t add the constant 1 column
poly_model = Pipeline([
    ("poly_features", PolynomialFeatures(degree=2, include_bias=False)),
    ("lin_reg", LinearRegression())
])

poly_model.fit(X_train, y_train)
y_poly_pred = poly_model.predict(X_test)

rmse_poly = np.sqrt(mean_squared_error(y_test, y_poly_pred))
r2_poly = r2_score(y_test, y_poly_pred)

print(f"Polynomial RMSE: {rmse_poly:.4f}")
print(f"Polynomial R²:   {r2_poly:.4f}")